<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Understanding the Future of Explainable Artificial Intelligence (XAI) | Eduard Ionel Stan </title> <meta name="author" content="Eduard Ionel Stan"> <meta name="description" content="A Dive into XAI 2.0"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://eduardstan.github.io/blog/2024/xai2-manifesto/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Understanding the Future of Explainable Artificial Intelligence (XAI)",
            "description": "A Dive into XAI 2.0",
            "published": "August 02, 2024",
            "authors": [
              
              {
                "author": "Ionel Eduard Stan",
                "authorURL": "https://eduardstan.github.io/",
                "affiliations": [
                  {
                    "name": "Imaging and Vision Laboratory (IVL), Department of Informatics, Systemsn and Communication (DISCo), University of Milano-Bicocca",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Eduard</span> Ionel Stan </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/professional_activities/">professional activities </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Understanding the Future of Explainable Artificial Intelligence (XAI)</h1> <p>A Dive into XAI 2.0</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#the-human-face-of-ai">The Human Face of AI</a> </div> <div> <a href="#the-evolution-and-urgency-of-xai">The Evolution and Urgency of XAI</a> </div> <div> <a href="#making-ai-transparent">Making AI Transparent</a> </div> <ul> <li> <a href="#simplifying-complex-ideas">Simplifying Complex Ideas</a> </li> <li> <a href="#an-experiment">An Experiment</a> </li> </ul> <div> <a href="#deeper-analysis">Deeper Analysis</a> </div> <ul> <li> <a href="#strengths-and-innovations">Strengths and Innovations</a> </li> <li> <a href="#critical-perspective">Critical Perspective</a> </li> <li> <a href="#ethical-considerations-and-societal-impact">Ethical Considerations and Societal Impact</a> </li> </ul> <div> <a href="#connecting-the-dots">Connecting the Dots</a> </div> <div> <a href="#a-call-to-curiosity">A Call to Curiosity</a> </div> <div> <a href="#resources-for-further-exploration">Resources for Further Exploration</a> </div> <div> <a href="#let-s-talk-science">Let’s Talk Science</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="the-human-face-of-ai">The Human Face of AI</h2> <blockquote> <p>Imagine being told that a computer algorithm determined whether you qualify for a loan or made a critical decision in your healthcare.</p> <p><strong>It’s unsettling, right?</strong></p> </blockquote> <p>We, as humans, naturally want to understand why such decisions were made. This desire for transparency is what drives the field of Explainable Artificial Intelligence (XAI). But as AI systems grow more complex, the challenge of making them understandable grows, too. Enter XAI 2.0, a new wave of research aimed at making AI not just more intelligent but also more comprehensible to humans.</p> <hr> <h2 id="the-evolution-and-urgency-of-xai">The Evolution and Urgency of XAI</h2> <p>Explainable Artificial Intelligence has become a vital area of research as AI systems increasingly influence critical areas like finance, healthcare, and environmental management. The paper <a href="https://www.sciencedirect.com/science/article/pii/S1566253524000794" rel="external nofollow noopener" target="_blank">Explainable Artificial Intelligence (XAI) 2.0: A Manifesto of Open Challenges and Interdisciplinary Research Directions</a> by Longo et al. brings together various experts from various fields to address the urgent need for transparency in AI systems. As AI continues to permeate our lives, understanding its decisions is no longer optional—it’s essential. The authors highlight 28 open problems in XAI, categorized into nine distinct high-level areas, aiming to synchronize research efforts and propel the field forward.</p> <details><summary>Click here for details on the areas, describing their open problems and possible solutions.</summary> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/xai2-manifesto-480.webp 480w,/assets/img/xai2-manifesto-800.webp 800w,/assets/img/xai2-manifesto-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/xai2-manifesto.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> XAI 2.0 manifesto: High-level challenges. </div> <p>The following tables outline the open problems associated with each high-level problem, providing descriptions and possible solutions to address them.</p> <h3 id="1-creating-explanations-for-new-types-of-ai">1. <strong>Creating explanations for new types of AI</strong> </h3> <table> <thead> <tr> <th><strong>Open Problem</strong></th> <th><strong>Description</strong></th> <th><strong>Possible Solutions</strong></th> </tr> </thead> <tbody> <tr> <td><strong>1.1. Creating explanations for generative and large language models</strong></td> <td>Generative models and large language models (LLMs) like GPT are complex, high-dimensional, and capable of producing diverse outputs, making it difficult to explain how they arrive at specific outputs.</td> <td>Develop new XAI techniques tailored for generative models and LLMs. Techniques include mechanistic interpretability, causal analysis, and visualization of latent spaces to understand learned representations.</td> </tr> <tr> <td><strong>1.2. Creating explanations for distributed and collaborative learning</strong></td> <td>Distributed and collaborative learning systems, such as federated learning, involve multiple models trained on decentralized data, complicating generating coherent, unified explanations.</td> <td>Create XAI frameworks that generate local explanations at individual nodes and aggregate them for global insights. Use privacy-preserving techniques like Multi-Party Computation (MPC) and differential privacy to protect sensitive data while explaining.</td> </tr> </tbody> </table> <h3 id="2-improving-current-xai-methods">2. <strong>Improving current XAI methods</strong> </h3> <table> <thead> <tr> <th><strong>Open Problem</strong></th> <th><strong>Description</strong></th> <th><strong>Possible Solutions</strong></th> </tr> </thead> <tbody> <tr> <td><strong>2.1. Augmenting and improving attribution methods</strong></td> <td>Attribution methods like saliency maps are widely used in XAI but suffer from issues such as sensitivity to input perturbations and difficulty in interpretation by non-experts.</td> <td>Combine attribution methods with other XAI techniques for a more robust explanation. Enhance the methods to reduce sensitivity to input variations and make them more interpretable by including explanations in natural language.</td> </tr> <tr> <td><strong>2.2. Augmenting and improving concept-based learning algorithms</strong></td> <td>Concept-based learning algorithms explain model predictions in terms of human-understandable concepts, but these methods often need help with generalization and applicability across domains.</td> <td>Integrate knowledge graphs and symbolic AI approaches with concept-based methods to enhance generalization. Develop techniques to automatically identify and learn new concepts from data, improving the flexibility and applicability of these models.</td> </tr> <tr> <td><strong>2.3. Removing artefacts in synthesis-based explanations</strong></td> <td>Synthesis-based explanations generate example inputs to illustrate model behavior, but these examples can contain artifacts that obscure the truly learned concepts.</td> <td>Reduce artifacts using state-of-the-art generative models like diffusion models. Implement comparison mechanisms to evaluate the accuracy of synthesized explanations against known reference inputs.</td> </tr> <tr> <td><strong>2.4. Creating robust explanations</strong></td> <td>Explanations can be fragile, changing significantly with minor input modifications, undermining their reliability and trustworthiness.</td> <td>Develop methods to evaluate the robustness of explanations under different conditions. Combine multiple explanation techniques to mitigate weaknesses in individual methods. Explore ante-hoc explainable models that inherently produce robust explanations.</td> </tr> </tbody> </table> <h3 id="3-clarifying-the-use-of-concepts-in-xai">3. <strong>Clarifying the use of concepts in XAI</strong> </h3> <table> <thead> <tr> <th><strong>Open Problem</strong></th> <th><strong>Description</strong></th> <th><strong>Possible Solutions</strong></th> </tr> </thead> <tbody> <tr> <td><strong>3.1. Elucidating the main concepts</strong></td> <td>There needs to be more consensus on pivotal terms like explainability, interpretability, and transparency in XAI, which can lead to confusion and inconsistency in research and application.</td> <td>Standardize terminology by creating a shared glossary or taxonomy of XAI terms. Engage in interdisciplinary discussions to align definitions and ensure they are applied consistently across different domains.</td> </tr> <tr> <td><strong>3.2. Clarifying the relationship between XAI and trustworthiness</strong></td> <td>The relationship between explainability and trustworthiness needs to be better understood, which hinders the development of truly trustworthy AI systems.</td> <td>Conduct interdisciplinary research to explore how XAI contributes to trustworthiness in AI. Develop frameworks integrating XAI with other trustworthiness criteria, such as fairness and robustness.</td> </tr> <tr> <td><strong>3.3. Finding a useful account of understanding</strong></td> <td>Understanding in XAI is often assumed rather than explicitly defined, making it difficult to measure and evaluate whether explanations genuinely enhance user comprehension.</td> <td>Drawing from psychology and philosophy, develop a clear conceptual framework for understanding XAI. Design empirical studies to test and refine this framework in practical applications.</td> </tr> </tbody> </table> <h3 id="4-evaluating-xai-methods-and-explanations">4. <strong>Evaluating XAI methods and explanations</strong> </h3> <table> <thead> <tr> <th><strong>Open Problem</strong></th> <th><strong>Description</strong></th> <th><strong>Possible Solutions</strong></th> </tr> </thead> <tbody> <tr> <td><strong>4.1. Facilitating human evaluation of explanations</strong></td> <td>Many XAI methods need more rigorous human evaluation, making it unclear whether they are genuinely helpful or comprehensible to end-users.</td> <td>Develop standardized protocols for user studies that evaluate the effectiveness and comprehensibility of XAI methods. Incorporate insights from HCI, psychology, and social sciences into the design of these studies.</td> </tr> <tr> <td><strong>4.2. Creating an evaluation framework for XAI methods</strong></td> <td>There is no universally accepted framework for evaluating XAI methods, leading to inconsistent quality and effectiveness assessments.</td> <td>Develop a comprehensive evaluation framework that includes fidelity, usability, and trustworthiness metrics. Encourage the adoption of standardized evaluation tools like Quantus across different XAI applications.</td> </tr> <tr> <td><strong>4.3. Overcoming limitations of studies with humans</strong></td> <td>Human studies in XAI are often limited by sample size, diversity, and reproducibility, which can lead to biased or unreliable conclusions.</td> <td>Synthetic data and virtual participants can supplement human studies and increase diversity. Standardize methodologies and statistical analyses to improve the reliability and generalizability of study findings.</td> </tr> </tbody> </table> <h3 id="5-supporting-the-human-centeredness-of-explanations">5. <strong>Supporting the human-centeredness of explanations</strong> </h3> <table> <thead> <tr> <th><strong>Open Problem</strong></th> <th><strong>Description</strong></th> <th><strong>Possible Solutions</strong></th> </tr> </thead> <tbody> <tr> <td><strong>5.1. Creating human-understandable explanations</strong></td> <td>Many XAI methods produce technically accurate explanations that are difficult for non-expert users to understand and use.</td> <td>Design explanations more aligned with human cognitive processes, using natural language and visual aids. Incorporate user feedback to refine and improve the accessibility of explanations.</td> </tr> <tr> <td><strong>5.2. Facilitating explainability with concept-based explanations</strong></td> <td>Concept-based explanations must bridge the gap between technical accuracy and human intuition, ensuring that explanations are meaningful and valuable.</td> <td>Integrate symbolic reasoning with concept-based explanations to enhance their interpretability. Develop tools that allow users to interactively explore and understand the concepts underlying model predictions.</td> </tr> <tr> <td><strong>5.3. Addressing explanations divorced from reality</strong></td> <td>Explanations that are technically correct but do not reflect real-world causality or logic can be misleading and reduce trust in AI systems.</td> <td>Ensure that explanations are grounded in real-world logic and causality by validating them against known outcomes and expert knowledge. Use causal inference techniques to improve the alignment between explanations and real-world processes.</td> </tr> <tr> <td><strong>5.4. Uncovering causality for actionable explanations</strong></td> <td>Understanding causality in AI models is critical for making explanations actionable, but current methods often fall short in this area.</td> <td>Develop methods that explicitly model and reveal causal relationships within AI systems. Combine XAI with causal inference tools to create explanations that not only describe correlations but also indicate potential interventions.</td> </tr> </tbody> </table> <h3 id="6-supporting-the-multi-dimensionality-of-explainability">6. <strong>Supporting the multi-dimensionality of explainability</strong> </h3> <table> <thead> <tr> <th><strong>Open Problem</strong></th> <th><strong>Description</strong></th> <th><strong>Possible Solutions</strong></th> </tr> </thead> <tbody> <tr> <td><strong>6.1. Creating multi-faceted explanations</strong></td> <td>AI decisions are often complex and multi-dimensional, requiring explanations that capture this complexity and present it in a user-friendly manner.</td> <td>Design XAI systems that provide layered explanations, allowing users to drill down from simple overviews to detailed insights. Implement multi-modal explanations that combine text, visuals, and interactive elements.</td> </tr> <tr> <td><strong>6.2. Enabling interdisciplinary work in XAI</strong></td> <td>XAI research and application require input from diverse fields, but differences in terminology, methodology, and goals often hinder interdisciplinary collaboration.</td> <td>Create interdisciplinary consortia or research groups focused on XAI, facilitating communication and collaboration across fields. Develop shared frameworks and tools that accommodate the needs of different disciplines.</td> </tr> </tbody> </table> <h3 id="7-adjusting-xai-methods-and-explanations">7. <strong>Adjusting XAI methods and explanations</strong> </h3> <table> <thead> <tr> <th><strong>Open Problem</strong></th> <th><strong>Description</strong></th> <th><strong>Possible Solutions</strong></th> </tr> </thead> <tbody> <tr> <td><strong>7.1. Adjusting explanations to different stakeholders</strong></td> <td>Different stakeholders (e.g., developers, end-users, regulators) have different needs and expectations from AI explanations, requiring tailored approaches.</td> <td>Develop customizable XAI systems that adjust explanations based on the stakeholder’s role, expertise, and goals. Implement user profiling to adapt explanations dynamically based on real-time user feedback.</td> </tr> <tr> <td><strong>7.2. Adjusting explanations to different domains</strong></td> <td>AI systems are used across diverse domains, each with specific requirements for explanations.</td> <td>Create domain-specific XAI tools that consider each application area’s unique needs and challenges. Collaborate with domain experts to ensure that explanations are relevant and accurate.</td> </tr> <tr> <td><strong>7.3. Adjusting explanations to different goals</strong></td> <td>The purpose of AI explanations can vary from understanding a decision to auditing an AI system, requiring flexibility in XAI approaches.</td> <td>Develop XAI methods that can shift focus based on the intended goal, such as providing detailed audit trails for compliance or simplified explanations for general users. Incorporate goal-based metrics into the evaluation of explanations.</td> </tr> </tbody> </table> <h3 id="8-mitigating-the-negative-impact-of-xai">8. <strong>Mitigating the negative impact of XAI</strong> </h3> <table> <thead> <tr> <th><strong>Open Problem</strong></th> <th><strong>Description</strong></th> <th><strong>Possible Solutions</strong></th> </tr> </thead> <tbody> <tr> <td><strong>8.1. Mitigating failed support by XAI</strong></td> <td>XAI methods sometimes fail to provide adequate support, leading to poor decision-making or a false sense of security among users.</td> <td>Develop fallback mechanisms that alert users to potential explanation failures and provide alternative forms of support. Based on user feedback and performance metrics, regularly update and improve XAI methods.</td> </tr> <tr> <td><strong>8.2. Devising criteria for the falsifiability of explanations</strong></td> <td>Explanations should be falsifiable, meaning they can be tested and proven wrong if incorrect, but many current methods still need to meet this standard.</td> <td>Implement mechanisms for testing and validating explanations, ensuring they are plausible and verifiable. Develop standards and guidelines for falsifiability in XAI.</td> </tr> <tr> <td><strong>8.3. Securing explanations from being abused by malicious human agents</strong></td> <td>Malicious actors can exploit XAI systems to manipulate or deceive, posing a risk to security and trust.</td> <td>Implement security measures to protect XAI systems from tampering and abuse. Monitor explanations for signs of manipulation and develop tools to detect and prevent such abuses.</td> </tr> <tr> <td><strong>8.4. Securing explanations from being abused by malicious superintelligent agents</strong></td> <td>As AI systems become more advanced, there is a potential risk that the AI itself could manipulate explanations for harmful purposes.</td> <td>Research and develop safeguards that prevent superintelligent agents from generating misleading or harmful explanations. To mitigate these risks, incorporate AI ethics and safety protocols into XAI design.</td> </tr> </tbody> </table> <h3 id="9-improving-the-societal-impact-of-xai">9. <strong>Improving the societal impact of XAI</strong> </h3> <table> <thead> <tr> <th><strong>Open Problem</strong></th> <th><strong>Description</strong></th> <th><strong>Possible Solutions</strong></th> </tr> </thead> <tbody> <tr> <td><strong>9.1. Facilitating originality attribution of AI-generated data and plagiarism detection</strong></td> <td>As AI systems increasingly generate content, it becomes challenging to attribute originality and detect plagiarism.</td> <td>Develop tools and algorithms that can track the provenance of AI-generated content and detect instances of plagiarism. Collaborate with legal and academic institutions to standardize methods for originality attribution.</td> </tr> <tr> <td><strong>9.2. Facilitating the right to be forgotten</strong></td> <td>The right to be forgotten is a legal right in many jurisdictions, but it is not easy to enforce in AI systems that store and process large amounts of data.</td> <td>Implement XAI tools that identify and selectively remove personal data from AI systems, ensuring compliance with privacy laws. Use privacy-preserving techniques like differential privacy and data anonymization to support this process.</td> </tr> <tr> <td><strong>9.3. Addressing the power imbalance between individuals and companies</strong></td> <td>Large companies often control AI systems, creating a power imbalance that can disadvantage individuals.</td> <td>Develop XAI methods that enhance transparency and give individuals more control over how AI systems use their data. Advocate for stronger regulations and policies that protect individuals’ rights in AI interactions.</td> </tr> </tbody> </table> </details> <hr> <h2 id="making-ai-transparent">Making AI Transparent</h2> <h3 id="simplifying-complex-ideas">Simplifying Complex Ideas</h3> <p>At the core of XAI is the need to explain decisions made by AI systems in ways that humans can understand. This could involve breaking down how a model arrived at a particular decision or providing insights into the model’s inner workings. Think of an AI model as a recipe: the ingredients are the data, the cooking process is the algorithm, and the final dish is the model’s decision. XAI aims to clarify the recipe so we know why the dish tastes like it does.</p> <p>To address this, the paper discusses various explanation methods, such as attribution methods that highlight which parts of the input data are most influential in the model’s decision and concept-based methods that explain decisions in terms of human-understandable concepts. The challenge lies in making these explanations both accurate and accessible.</p> <h3 id="an-experiment">An Experiment</h3> <p>To understand XAI better, try a simple thought experiment:</p> <blockquote> <p>Imagine you’re a teacher grading an essay.</p> <p>Providing constructive feedback is challenging if you see only the final grade without any comments or highlights on what the student did right or wrong.</p> <p>However, if the essay has detailed annotations explaining each grading decision, it becomes easier to provide feedback.</p> </blockquote> <p>This process is similar to what XAI aims to do with AI models; it annotates and explains the decisions so we can understand and trust them.</p> <hr> <h2 id="deeper-analysis">Deeper Analysis</h2> <h3 id="strengths-and-innovations">Strengths and Innovations</h3> <p>One of the paper’s key strengths is its comprehensive approach to addressing the challenges in XAI. The authors propose innovative methods like mechanistic interpretability, which involves reverse-engineering neural networks to understand their inner workings. This approach is akin to taking apart a clock to see how the gears fit together and make it tick, providing deep insights into the AI’s decision-making process.</p> <p>Additionally, I was particularly intrigued by the discussion of symbolic approaches to XAI, such as concept-based methods. These methods seek to explain AI decisions using human-understandable concepts, which can make AI more relatable and comprehensible. Related to this, the authors also highlight neuro-symbolic approaches, which combine symbolic reasoning with neural networks. This hybrid approach could bridge the gap between traditional, logic-based AI and the more recent data-driven methods, offering explanations that are both logically sound and grounded in data.</p> <p>Moreover, the paper emphasizes the importance of interdisciplinary collaboration in XAI, bringing together experts in computer science, philosophy, psychology, and beyond to tackle these challenges. This multidisciplinary approach is crucial, as the explanations produced by XAI need to be meaningful not just to AI researchers but to a broad audience, including policymakers, end-users, and the general public.</p> <h3 id="critical-perspective">Critical Perspective</h3> <p>However, the paper also highlights significant limitations in the current state of XAI. One major issue is the need for more explanation for new types of AI models, such as generative and large language models. These models are like black boxes with numerous parameters, making dissecting and explaining their decisions challenging.</p> <p>Another limitation is the potential for XAI methods to produce misleading explanations. For example, while practical, popular methods like Shapley values can sometimes create the illusion of understanding without truly capturing the model’s decision process. This raises ethical concerns about the reliability of explanations and the potential for misusing or misunderstanding of these methods.</p> <h3 id="ethical-considerations-and-societal-impact">Ethical Considerations and Societal Impact</h3> <p>The ethical implications of XAI are profound. As AI systems increasingly make decisions that affect our lives, ensuring these decisions are transparent and understandable is critical for building trust. The paper discusses the ethical principle of “explainability” as a core requirement for trustworthy AI, alongside other principles like fairness and harm prevention. The authors call for a balanced approach, where explanations are not just technically sound but also aligned with ethical standards, ensuring that AI systems can be held accountable.</p> <hr> <h2 id="connecting-the-dots">Connecting the Dots</h2> <p>Reflecting on this research, it’s clear that XAI is not just about making AI more transparent; it’s about bridging the gap between machines and humans.</p> <blockquote> <p>What surprised me most was that we still grapple with fundamental questions about trust and understanding, even with all our technological advancements.</p> </blockquote> <p>This paper connects to broader trends in AI governance and ethics, where the focus is increasingly on making AI systems not just more powerful but more responsible.</p> <p>The authors also mention that the field of XAI is “fermenting,” which I found to be an apt description. XAI research’s rapid evolution and expansion have led to a situation where we are only scratching the surface of what is possible. The paper offers a slice of the vast literature on XAI, but the reality is that there is much more to explore. This fermentation process suggests that XAI is at a critical juncture with significant potential for growth and impact.</p> <p>The discussions in this paper also raise important questions about the future of AI:</p> <blockquote> <p>How do we ensure that as AI becomes more complex, it doesn’t become more opaque?</p> <p>How do we balance the need for powerful AI with transparency?</p> </blockquote> <p>These questions will shape the future of AI and its role in society.</p> <hr> <h2 id="a-call-to-curiosity">A Call to Curiosity</h2> <p>As you think about the role of AI in your life, consider this:</p> <blockquote> <p><strong>How much do you trust the decisions made by machines?</strong></p> <p><strong>What would make you trust them more?</strong></p> </blockquote> <p>These are the kinds of questions that drive research in XAI. Please explore this field further, whether by reading more about AI ethics, experimenting with AI tools, or simply asking more questions about how the technology you use every day works.</p> <hr> <h2 id="resources-for-further-exploration">Resources for Further Exploration</h2> <p>For those interested in delving deeper into XAI, here are some resources:</p> <ul> <li> <a href="https://www.sciencedirect.com/science/article/pii/S1566253523001148" rel="external nofollow noopener" target="_blank">Explainable Artificial Intelligence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence</a> by Ali et al. for an overview of current research and trends in XAI.</li> <li> <a href="https://link.springer.com/article/10.1007/s10618-022-00867-8" rel="external nofollow noopener" target="_blank">A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts</a> by Schwalbe and Finzel for a meta-review of surveys on XAI’s methods and concepts.</li> <li> <a href="https://www.sciencedirect.com/science/article/pii/S0004370218305988" rel="external nofollow noopener" target="_blank">Explanation in artificial intelligence: Insights from the social sciences</a> by Miller for insights from social sciences on XAI.</li> </ul> <hr> <h2 id="lets-talk-science">Let’s Talk Science</h2> <p>I invite you to share your thoughts, questions, and experiences with AI in the comments. Let’s build a community where we can explore these topics together, making science accessible, exciting, and relevant to everyone.</p> <h3 id="a-personal-note">A Personal Note</h3> <p>This is my first blog post, and I’m eager to create a platform for shared learning with the community. I invite you to be kind and give constructive feedback, as I’m committed to modifying, integrating, adjusting, and improving this post and future ones based on your curiosity, needs, and insights. Your feedback is invaluable as we collectively explore and communicate the fascinating world of AI.</p> <hr> <h2 id="conclusion">Conclusion</h2> <p>Explainable AI is not just a technical challenge; it’s a societal one. As we move into the future, ensuring that AI systems are transparent, trustworthy, and aligned with human values will be critical. XAI 2.0 is a step towards that future, but it’s a journey that requires all of us—researchers, practitioners, and the public—to engage with the science and the questions it raises.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"eduardstan/eduardstan.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Eduard Ionel Stan. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: January 13, 2025. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0104MW94M6"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0104MW94M6");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-cv",title:"cv",description:"A succint overview. The PDF file is not ready yet to be downloaded! Last updated July 27, 2024.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-professional-activities",title:"professional activities",description:"a selection of professional activities",section:"Navigation",handler:()=>{window.location.href="/professional_activities/"}},{id:"nav-news",title:"news",description:"",section:"Navigation",handler:()=>{window.location.href="/news/"}},{id:"post-mastering-latex",title:"Mastering LaTeX",description:"An In-Depth Guide to Common Mistakes and Advanced Techniques",section:"Posts",handler:()=>{window.location.href="/blog/2025/latex-mistakes-and-solutions/"}},{id:"post-understanding-the-future-of-explainable-artificial-intelligence-xai",title:"Understanding the Future of Explainable Artificial Intelligence (XAI)",description:"A Dive into XAI 2.0",section:"Posts",handler:()=>{window.location.href="/blog/2024/xai2-manifesto/"}},{id:"news-invited-and-accepted-to-serve-on-the-program-committee-pc-for-the-39th-association-for-the-advancement-of-artificial-intelligence-conference-on-artificial-intelligence-aaai2025-https-aaai-org-conference-aaai-aaai-25",title:"Invited and accepted to serve on the **Program Committee (PC)** for the [39th...",description:"",section:"News"},{id:"news-started-a-new-position-as-assistant-professor-at-the-imaging-and-vision-laboratory-ivl-http-www-ivl-disco-unimib-it-in-the-department-of-informatics-systems-and-communication-disco-https-www-disco-unimib-it-of-the-university-of-milan-bicocca-https-www-unimib-it",title:"Started a new position as **Assistant Professor** at the [Imaging and Vision Laboratory...",description:"",section:"News"},{id:"news-modalfp-growth-efficient-extraction-of-modal-association-rules-from-non-tabular-data-has-been-accepted-for-publication-at-the-25th-italian-conference-on-theoretical-computer-science-ictcs2024-https-ictcs2024-di-unito-it",title:"**ModalFP-Growth: Efficient Extraction of Modal Association Rules from Non-Tabular Data** has been accepted...",description:"",section:"News"},{id:"news-symbolic-audio-classification-via-modal-decision-tree-learning-has-been-accepted-for-publication-at-the-3rd-italian-conference-on-big-data-and-data-science-itadata2024-https-www-itadata-it",title:"**Symbolic Audio Classification via Modal Decision Tree Learning** has been accepted for publication...",description:"",section:"News"},{id:"news-new-post-understanding-the-future-of-explainable-artificial-intelligence-xai-a-dive-into-xai-2-0-https-eduardstan-github-io-blog-2024-xai2-manifesto",title:"New post: [Understanding the Future of Explainable Artificial Intelligence (XAI)---A Dive into XAI...",description:"",section:"News"},{id:"news-neural-symbolic-temporal-decision-trees-for-multivariate-time-series-classification-https-www-sciencedirect-com-science-article-pii-s0890540124000749-is-online",title:"[Neural-symbolic temporal decision trees for multivariate time series classification](https://www.sciencedirect.com/science/article/pii/S0890540124000749) is online!",description:"",section:"News"},{id:"news-invited-and-accepted-to-serve-as-a-reviewer-for-the-ieee-journal-of-biomedical-health-informatics-https-www-embs-org-jbhi-journal",title:"Invited and accepted to serve as a **Reviewer** for the [IEEE Journal of...",description:"",section:"News"},{id:"news-invited-and-accepted-to-serve-as-a-reviewer-for-the-13th-international-conference-on-learning-representations-iclr2025-https-iclr-cc-conferences-2025",title:"Invited and accepted to serve as a **Reviewer** for the [13th International Conference...",description:"",section:"News"},{id:"news-fitting-s-style-many-valued-interval-temporal-logic-tableau-system-theory-and-implementation-https-drops-dagstuhl-de-entities-document-10-4230-lipics-time-2024-7-is-online",title:"[Fitting\u2019s Style Many-Valued Interval Temporal Logic Tableau System: Theory and Implementation](https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.TIME.2024.7) is online!...",description:"",section:"News"},{id:"news-invited-and-accepted-to-serve-on-the-area-chair-ac-for-the-2025-international-joint-conference-on-neural-networks-ijcnn2025-https-2025-ijcnn-org",title:"Invited and accepted to serve on the **Area Chair (AC)** for the [2025...",description:"",section:"News"},{id:"news-invited-and-accepted-to-serve-as-a-reviewer-for-the-engineering-applications-of-artificial-intelligence-https-www-sciencedirect-com-journal-engineering-applications-of-artificial-intelligence-journal",title:"Invited and accepted to serve as a **Reviewer** for the [Engineering Applications of...",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%69%6F%6E%65%6C%65%64%75%61%72%64.%73%74%61%6E@%75%6E%69%6D%69%62.%69%74","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=8BMCQS8AAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/eduardstan","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/edust","_blank")}},{id:"socials-dblp",title:"DBLP",section:"Socials",handler:()=>{window.open("https://dblp.org/pid/208/2243.html","_blank")}},{id:"socials-instagram",title:"Instagram",section:"Socials",handler:()=>{window.open("https://instagram.com/0xedu","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>